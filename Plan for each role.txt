Plan for each role:
Detailed Step-by-Step Plans for Each Group Member
________________


Group Member 1: Project Lead & Documentation
Step 1: Set Up GitHub Repository (30 minutes)
1. Go to GitHub.com and sign in
2. Click the "+" icon → "New repository"
3. Name it: tv-shows-dataset-api (or similar)
4. Make it Public
5. Check "Add a README file"
6. Click "Create repository"
7. Go to Settings → Collaborators → Add people
8. Search for username: cfb3 and add as collaborator
Step 2: Create Project Structure (20 minutes)
1. Clone the repository to your local machine:
bash
   git clone https://github.com/your-username/tv-shows-dataset-api.git
   cd tv-shows-dataset-api
2. Create the /project_files directory:
bash
   mkdir project_files
3. Create initial README structure:
bash
   touch README.md
4. Commit and push:
bash
   git add .
   git commit -m "Initial project structure"
   git push origin main
Step 3: Add Project Planning Document (1 hour)
1. Copy the Project Planning Document I created above
2. Save it as /project_files/project_plan.md
3. Review and customize it if needed for your specific needs
4. Add, commit, and push:
bash
   git add project_files/project_plan.md
   git commit -m "Add project planning document"
   git push origin main
Step 4: Coordinate Team Communication (Ongoing)
1. Create a group chat (Discord, Slack, WhatsApp, or Teams)
2. Schedule at least 2 team meetings during Alpha Sprint
3. Document the primary forms of communication used
4. Keep notes of what was discussed in each meeting
5. Track who's completing which tasks
Step 5: Write README.md (2 hours)
1. Open README.md in your text editor
2. Add project title and description
3. Add section: Alpha Sprint Contributions 
   * List each member's name
   * Describe what each member contributed
4. Add section: Alpha Sprint Meetings 
   * List communication methods (Discord, Zoom, etc.)
   * Describe when and how often the team met
   * Summarize what was discussed
5. Add section: Alpha Sprint Comments 
   * List any issues encountered
   * Note any interesting discoveries
   * Mention code decisions or challenges
6. Commit and push changes
Step 6: Final Review and Submission (1 hour)
1. Verify all files are in the correct locations
2. Check that cfb3 is added as collaborator
3. Ensure all team members' contributions are documented
4. Test that all links in README work
5. Submit the GitHub repository link
Total Estimated Time: 5-6 hours
________________


Group Member 2: Database Designer
Step 1: Understand the Dataset (1 hour)
1. Download and open tv_last1years.csv
2. Examine the columns and data types
3. Identify relationships between data entities: 
   * TV Shows (main entity)
   * Genres (multiple per show)
   * Actors (multiple per show)
   * Networks (multiple per show)
   * Studios (multiple per show)
   * Creators (multiple per show)
4. Note which fields are semicolon-separated lists
5. Identify primary and foreign keys
Step 2: Create ER Diagram (2 hours)
1. Use one of these tools: 
   * Draw.io (https://app.diagrams.net/)
   * Lucidchart (free account)
   * dbdiagram.io
   * Or use the Mermaid diagram I created above
2. Create entities: 
   * tv_shows (main table)
   * genres, creators, networks, studios, actors (lookup tables)
   * Junction tables for many-to-many relationships
3. Define attributes for each entity
4. Show relationships with cardinality (1:many, many:many)
5. Export as image (PNG/JPG) or keep as Mermaid code
6. Save as /project_files/er_diagram.png or /project_files/er_diagram.mmd
Step 3: Write SQL Schema Creation Script (2 hours)
1. Create file: /project_files/init_database.sql
2. Write DROP TABLE statements (for re-running script)
3. Write CREATE TABLE statements for all tables: 
   * Start with main tv_shows table
   * Create lookup tables (genres, creators, etc.)
   * Create junction tables (show_genres, show_cast, etc.)
4. Add PRIMARY KEY constraints
5. Add FOREIGN KEY constraints
6. Add UNIQUE constraints where needed
7. Use the SQL script I created above as your template
Step 4: Write Data Migration Queries (2 hours)
1. In the same SQL file, add commented migration steps
2. Write queries to: 
   * Load CSV data into temp table
   * Extract unique genres and insert into genres table
   * Extract unique actors and insert into actors table
   * Parse semicolon-separated fields
   * Create relationships in junction tables
3. Add example queries for: 
   * Splitting "Crime; Drama" into individual genres
   * Extracting Actor 1-10 into normalized actors table
   * Linking shows to networks
Step 5: Add Indexes for Performance (30 minutes)
1. Add CREATE INDEX statements for: 
   * tv_shows.name (for searching by title)
   * tv_shows.popularity (for sorting by popularity)
   * tv_shows.tmdb_rating (for filtering by rating)
   * Foreign keys in junction tables
2. Add comments explaining why each index is needed
Step 6: Test and Document (1 hour)
1. If you have PostgreSQL installed locally, test the script:
bash
   psql -U postgres -f project_files/init_database.sql
2. Verify tables are created correctly
3. Add a comment block at the top of the SQL file explaining: 
   * What the script does
   * Prerequisites (PostgreSQL version)
   * How to run it
4. Add example SELECT queries at the bottom
5. Commit and push to GitHub
Step 7: Create README for Database (30 minutes)
1. Create /project_files/database_README.md
2. Explain the database schema
3. List all tables and their purposes
4. Describe relationships
5. Provide instructions for running the SQL script
Total Estimated Time: 8-9 hours
________________


Group Member 3: API Designer & Swagger Documentation
Step 1: Review Project Requirements (1 hour)
1. Read the Project Planning Document thoroughly
2. Understand what functionality the API needs to provide
3. Review the ER diagram to understand data structure
4. List all the endpoints needed based on requirements: 
   * Get all shows
   * Get show by ID
   * Search shows by name
   * Filter by genre
   * Filter by network
   * Get shows by actor
   * Get popular shows
   * etc.
Step 2: Design API Endpoints (2 hours)
1. Create a list of all endpoints with HTTP methods:
   GET    /api/shows              - Get all shows
   GET    /api/shows/{id}         - Get show by ID
   GET    /api/shows/search       - Search shows
   GET    /api/genres             - Get all genres
   GET    /api/shows/genre/{name} - Get shows by genre
   GET    /api/actors             - Get all actors
   GET    /api/actors/{id}/shows  - Get shows by actor
   GET    /api/networks           - Get all networks
   etc.
2. For each endpoint, define: 
   * Path parameters (e.g., {id})
   * Query parameters (e.g., ?limit=10&page=1)
   * Response format (JSON structure)
   * Possible error responses
Step 3: Set Up Swagger YAML File (30 minutes)
1. Create file: /project_files/api_spec.yaml
2. Start with basic OpenAPI 3.0 structure:
yaml
   openapi: 3.0.0
   info:
     title: TV Shows Database API
     version: 1.0.0
     description: API for accessing TV show information
   servers:
     - url: http://localhost:3000/api
       description: Development server
Step 4: Define Data Schemas (1.5 hours)
1. In the components/schemas section, define all data models: 
   * TVShow (with all properties from database)
   * Genre
   * Actor
   * Network
   * Studio
   * Creator
   * Error (for error responses)
2. Specify data types (string, integer, number, array)
3. Mark required fields
4. Add descriptions for each property
5. Add example values
Step 5: Document Each Endpoint (3 hours)
1. For each endpoint, add:
yaml
   paths:
     /shows:
       get:
         summary: Get all TV shows
         description: Retrieve a paginated list of all TV shows
         parameters:
           - name: limit
             in: query
             description: Number of results to return
             schema:
               type: integer
               default: 20
         responses:
           '200':
             description: Successful response
             content:
               application/json:
                 schema:
                   type: array
                   items:
                     $ref: '#/components/schemas/TVShow'
2. Add all query parameters with descriptions
3. Define response schemas
4. Add error responses (400, 404, 500)
5. Include example responses
Step 6: Add Security and Tags (30 minutes)
1. Add tags to group related endpoints:
yaml
   tags:
     - name: Shows
       description: TV show operations
     - name: Actors
       description: Actor operations
2. Add security schemes (even if not implementing auth yet):
yaml
   components:
     securitySchemes:
       bearerAuth:
         type: http
         scheme: bearer
Step 7: Validate and Test (1 hour)
1. Use Swagger Editor (https://editor.swagger.io/) to validate YAML
2. Paste your YAML and check for errors
3. Review the generated documentation
4. Test example requests
5. Make sure all schemas are properly referenced
6. Ensure consistent naming conventions
Step 8: Document Usage (30 minutes)
1. Add comments in YAML file explaining complex parts
2. Create /project_files/api_README.md with: 
   * How to view the Swagger documentation
   * How to test endpoints
   * Authentication requirements (future)
   * Rate limiting information (future)
3. Commit and push to GitHub
Total Estimated Time: 9-10 hours
________________


Group Member 4: Cloud Hosting Research & Testing
Step 1: Research Hosting Platforms (2 hours)
1. Research at least 3 platforms that offer free tiers: 
   * Render (https://render.com)
   * Railway (https://railway.app)
   * Fly.io (alternative option)
   * Heroku (now paid, but worth mentioning)
2. For each platform, document: 
   * Free tier specifications
   * PostgreSQL database options
   * Deployment methods
   * Geographic regions
   * SSL certificate support
3. Check GitHub Student Developer Pack benefits for each
Step 2: Create Simple HelloWorld API (1 hour)
1. Create a simple Node.js/Express app for testing:
javascript
   // server.js
   const express = require('express');
   const app = express();
   const PORT = process.env.PORT || 3000;


   app.get('/', (req, res) => {
     res.json({ 
       message: 'Hello World from TV Shows API!',
       timestamp: new Date().toISOString(),
       environment: process.env.NODE_ENV || 'development'
     });
   });


   app.get('/health', (req, res) => {
     res.json({ status: 'healthy' });
   });


   app.listen(PORT, () => {
     console.log(`Server running on port ${PORT}`);
   });
2. Create package.json:
json
   {
     "name": "tv-shows-api-hello",
     "version": "1.0.0",
     "main": "server.js",
     "scripts": {
       "start": "node server.js"
     },
     "dependencies": {
       "express": "^4.18.2"
     }
   }
3. Test locally:
bash
   npm install
   npm start
Step 3: Deploy to Render (1.5 hours)
1. Create account at https://render.com (use GitHub login)
2. Create new Web Service
3. Connect to your GitHub repository
4. Configure: 
   * Build Command: npm install
   * Start Command: npm start
   * Environment: Node
5. Deploy and wait for build
6. Test the deployed URL
7. Document: 
   * Deployment URL
   * Time to deploy
   * Cold start time
   * Any issues encountered
8. Take screenshots of: 
   * Render dashboard
   * Successful deployment
   * API response in browser
Step 4: Deploy to Railway (1.5 hours)
1. Create account at https://railway.app
2. Create new project
3. Deploy from GitHub repository
4. Configure environment variables if needed
5. Wait for deployment
6. Test the deployed URL
7. Document: 
   * Deployment URL
   * Time to deploy
   * Performance observations
   * Credit usage
8. Take screenshots
Step 5: Compare and Test Both Platforms (1 hour)
1. Test both deployments: 
   * Make multiple requests to each
   * Test cold start times (wait 15+ min, then request)
   * Check response times
   * Test any database connection (if added)
2. Document findings: 
   * Which was easier to set up?
   * Which has better performance?
   * Which has better free tier limits?
3. Note any problems or limitations
Step 6: Write Technical Hosting Document (2 hours)
1. Create /project_files/hosting_options.md
2. For each platform, write: 
   * Overview: Brief description
   * Free Tier Specs: Detailed specifications
   * Pros: List all advantages (use ✅)
   * Cons: List all disadvantages (use ❌)
   * GitHub Student Benefits: What extras you get
   * Deployment Steps: Step-by-step instructions
   * Demo Results: Include URLs and performance data
3. Add comparison table summarizing both platforms
4. Include recommendations for different use cases
5. Use the document I created above as your template
Step 7: Add Screenshots and Evidence (30 minutes)
1. Create /project_files/deployment_screenshots/ directory
2. Add screenshots showing: 
   * Both platforms' dashboards
   * Successful deployments
   * API responses
   * Performance metrics
3. Reference these in your hosting document
Step 8: Clean Up and Document (30 minutes)
1. Add deployment URLs to README.md
2. Document any issues in Alpha Sprint Comments
3. Commit all files to GitHub
4. Verify deployments are still working
Total Estimated Time: 9-10 hours
________________


Group Member 5: Data Analysis & Quality Assurance
Step 1: Analyze the Dataset Thoroughly (2 hours)
1. Open tv_last1years.csv in Excel or Google Sheets
2. Document dataset statistics: 
   * Total number of rows (TV shows)
   * Number of columns
   * Date range (earliest to latest air dates)
3. For each column, analyze: 
   * Data type (text, number, date)
   * Required vs optional (count nulls)
   * Value format (especially semicolon-separated fields)
   * Example values
4. Identify data quality issues: 
   * Missing values
   * Inconsistent formats
   * Special characters
   * Duplicate entries
5. Create /project_files/dataset_analysis.md documenting findings
Step 2: Verify ER Diagram Matches Dataset (1 hour)
1. Review Group Member 2's ER diagram
2. Check that all CSV columns are represented in tables
3. Verify relationships make sense: 
   * Are genres properly separated?
   * Are actors properly normalized?
   * Are many-to-many relationships correct?
4. Look for missing entities or relationships
5. Provide feedback to Group Member 2 if changes needed
6. Document verification in /project_files/qa_checklist.md
Step 3: Review SQL Script (1.5 hours)
1. Read through the SQL initialization script
2. Verify: 
   * All tables from ER diagram are created
   * Data types are appropriate for CSV data
   * Foreign key relationships are correct
   * Indexes are on commonly queried fields
3. Check data migration queries: 
   * Do they handle semicolon-separated values?
   * Do they handle null values properly?
   * Are Actor 1-10 properly extracted?
4. If you have PostgreSQL installed, test the script
5. Document any issues or suggestions
Step 4: Validate Swagger API Design (1.5 hours)
1. Review Group Member 3's Swagger YAML file
2. Check that endpoints align with Project Planning Document
3. Verify: 
   * All required functionality is covered
   * Response schemas match database structure
   * Query parameters make sense
   * Error responses are defined
4. Use Swagger Editor to validate YAML syntax
5. Test that example requests make sense
6. Provide feedback on missing endpoints or improvements
Step 5: Review Hosting Documentation (1 hour)
1. Read Group Member 4's hosting document
2. Verify: 
   * At least 2 platforms are documented
   * Pros and cons are realistic
   * Deployment steps are clear
   * Demo URLs are working
3. Test the deployed HelloWorld APIs yourself
4. Document your experience accessing them
5. Note any additional pros/cons you discover
Step 6: Create Test Queries (1.5 hours)
1. Create /project_files/test_queries.sql
2. Write SQL queries to test the database design:
sql
   -- Test 1: Get all shows with their genres
   SELECT s.name, string_agg(g.genre_name, ', ') as genres
   FROM tv_shows s
   JOIN show_genres sg ON s.show_id = sg.show_id
   JOIN genres g ON sg.genre_id = g.genre_id
   GROUP BY s.show_id, s.name
   LIMIT 10;
   
   -- Test 2: Find top 10 most popular shows
   SELECT name, popularity, tmdb_rating
   FROM tv_shows
   ORDER BY popularity DESC
   LIMIT 10;
   
   -- Test 3: Get all actors in a specific show
   -- Add more test queries...
3. Write at least 10 test queries covering: 
   * Basic selects
   * Joins across tables
   * Filtering
   * Aggregations
   * Searches
4. Document what each query tests
Step 7: Review Project Planning Document (1 hour)
1. Read Group Member 1's project plan
2. Check that it's written for non-technical readers
3. Verify: 
   * All dataset features are covered
   * Use cases are realistic
   * Functionality descriptions are clear
   * Examples make sense
4. Suggest improvements for clarity
5. Check for typos and grammar
Step 8: Final QA and Documentation (1.5 hours)
1. Review entire repository structure:
   /
   ├── README.md
   ├── project_files/
   │   ├── project_plan.md
   │   ├── er_diagram.png (or .mmd)
   │   ├── init_database.sql
   │   ├── api_spec.yaml
   │   ├── hosting_options.md
   │   ├── dataset_analysis.md
   │   ├── test_queries.sql
   │   └── qa_checklist.md
2. Create comprehensive QA checklist in /project_files/qa_checklist.md: 
   * ✅ All required files present
   * ✅ cfb3 added as collaborator
   * ✅ ER diagram matches dataset
   * ✅ SQL script is valid
   * ✅ Swagger YAML is valid
   * ✅ 2+ hosting options documented
   * ✅ Demo deployments working
   * ✅ README.md complete
3. Help other members fix any issues you found
4. Write summary in Alpha Sprint Comments section
Total Estimated Time: 10-11 hours
________________


Timeline Recommendation
Week 1:
* Days 1-2: All members review requirements and dataset
* Days 3-4: Members 1, 2, 3 work on their primary deliverables
* Days 5-7: Members 4, 5 work on their primary deliverables
Week 2:
* Days 8-10: All members complete their sections
* Days 11-12: Member 5 does QA, everyone fixes issues
* Days 13-14: Member 1 compiles README, final review, submission